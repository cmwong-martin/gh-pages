---
title: Light RNN - compress your RNN model from 80GB to 50MB
---

### Problem ###
Large RNN model is common but it is **infeasible** to

 1. fit into the memory of GPU devices.
 2. host in mobile devices for efficient inferences.

### Example ###
Consider the ClueWeb dataset [1], the vocabulary contains over 10M words. Then, the size of the input embedding matrix will be **~40GB**. And consider the output embedding matrix and those weights between hidden layers, the RNN model will be **>80GB**, which is much exceeding the capacity of current GPU (8GB).

### Proposed Method ###
Consider the Figure 1., the left table is the traditional representation of each vocabulary. The middle table is the proposed two-component representation [2] for each vocabulary.  The right one shows how each vocabulary shares two components: a row vector and a column vector.

![example image](/gh-pages/table/twoComponentTable.png){: .center-image }


### Advantages ###
Using two-component representation to a vocabulary of $$|V|$$ unique words, only $$2{\sqrt {|V|}}$$ vectors are needed. Therefore, the proposed method only requires $$2{\sqrt {|V|}}$$ vectors to represent a vocabulary is far less than $$|V|$$ required by existing approaches.

### Disadvantages ###
The architecture of the proposed RNN model is shown in Figure 2 Left. The RNN model now needs to model two components for a single hidden layer: row $$X^r$$ and column $$X^c$$, leading to a more complex model.   

![example image](/gh-pages/figure/twoComponentRNN.png){: .center-image}

### Result ###

![example image](/gh-pages/table/twoComponentRNN2.png){: .center-image}

![example image](/gh-pages/table/twoComponentRNN3.png){: .center-image}


### Reference ###

1. Pomikálek, Jan, Milos Jakubícek, and Pavel Rychlý. "Building a 70 billion word corpus of English from ClueWeb." LREC. 2012.
2. Xiang Li, Tao Qin, Jian Yang, and Tie-Yan Liu, Lightrnn: Memory and computation-efficient recurrent neural networks, CoRR abs/1610.09893 (2016).
